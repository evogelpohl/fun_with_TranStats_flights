<prompt>
    <instructions>You are a world-class data scientist and data engineer who uses Python and Pandas.
        Your task is to make raw data from CSVs into an analysis-ready format by performing data
        cleaning, transformation, and feature engineering consistent with exploratory data analysis
        and data preparation steps in a Jupyter Notebook. Use vectorized Pandas over per-row loops.
        Organize work in a single notebook at notebooks/flights_eda_engineering.ipynb with clear
        markdown headings and a dedicated Globals & Configuration cell.</instructions>
    <context>The data are from the famous Bureau of Transportation Statistics (BTS) for on-time
        flight
        performance for all US flights. Note, unlike transactional data, these data will be used for
        statistical analysis and generalizing about flights performance by; carrier, airport, etc.</context>
    <input_data>There are 12 CSVs, one for each month, in the ../data/flights folder of this
        project's root. Each CSV maps to the data dictionary found here:
        https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ and also here in PDF format, in the
        /docs folder of this project's root</input_data>
    <code_formatting>Following these rules when creating the IPYNB Jupyter notebook:
        * Every cell should achieve one objective, preceeded with a MD cell with a title and what
        the following code cell does.
        * Avoid using overly complex syntax. Keep the code readable yet efficient.
        * Collect global variables and configurations in a separate cell for easy modification.
        * Ensure to use .head(3) or similar for lists at the end of each cell to preserve output in
        Jupyter and for quick
        human review.
        * If you create a cell for `helper functions`, fine, but if the function is only used for
        one engineering or feature-creation objective, include it in the same cell as the code that
        uses
        it to prevent needless scrolling.
        * Prefer categorical dtypes for small enumerations (e.g., daypart), and use nullable
        integers (Int8/Int64) where appropriate.</code_formatting>
    <data_formatting>
        * Standardize column names to lowercase with underscores instead of spaces.
        * Ensure dtypes are consistent with the data dictionary and acceptable when exported to a
        parquet file.
        * Use relative paths from the notebook: DATA_DIR = Path('../data'), with subfolders
        flights/, dims/, output/.</data_formatting>
    <feature_engineering_criteria>
        FE1: Compute time-of-day features.
        - Parse HHMM fields robustly (handle strings like '0856', '856.00', non-digits, and '2400').
        - Create daypart_sched from CRSDepTime and daypart_actual from DepTime using 6 buckets:
          night, early_morning, morning, afternoon, late_afternoon, evening. Use categorical dtype
          with that exact order.
        - Create deptimehour (0â€“23) from DepTime for fine-grained analysis.
        - Handle midnight rollover logically; for daypart bucketing, only time-of-day matters.

        FE2: Create origin_tier (airport size). Group all flights by origin and rank airports by
        volume; assign top 20% 'Tier 1', next 50% 'Tier 2', bottom 30% 'Tier 3'.

        FE3: Create filtering flags based on DepDel15. Create is_late_departure (1/0) and
        is_on_time_departure (1/0). Ensure consistency with depdelayminutes.

        FE4: Enrich with airport coordinates. Read ../data/dims/T_MASTER_CORD.csv (latest rows
        only) and join by airport IDs to produce origin_lat, origin_long, dest_lat, dest_long. Drop
        other joined columns.

        FE5: Add airline_name. Read ../data/dims/L_AIRLINE_ID.csv, parse Description like
        "Southwest Airlines Co.: WN" to extract airline_name, and join on IATA_CODE_Reporting_Airline.
        Keep only airline_name from the join.

        FE6: Create a stratified sample for fast analysis. Stratify by month, IATA_CODE_Reporting_Airline,
        and DepDel15 at a configurable SAMPLE_RATE (e.g., 10%). Set RANDOM_STATE for reproducibility.
        Export df_final to ../data/output/flights_2024_clean_sampled.parquet.
    </feature_engineering_criteria>
    <data_engineering_criteria>
        DE1: Key fields in this dataset are DepDelayMinutes and DepDel15. If the row has those
        fields blank/na/null, then remove the row.

        DE2: Columns that have >= 80% nulls, remove the col.

        DE3: Set Dtypes according to the data dictionary URL provided or use EDA to determine
        appropriate
        types.

        DE4: Use visuals consistent with Data Scientist best practices as they engineer data,
        showing
        multiple-plots or coorplots as needed to showcase proof of successful engineering mutations.

        DE5: Perform sanity checks to ensure distributions, count and other techniques employed by
        data
        scientists to ensure the data are valid and effective for secondary analysis.

        DE6: Final cleanup of redundant/ambiguous columns. After creating flags and joins, drop
        join-only IDs and ambiguous fields, including: DepDel15 (after flags), reporting_airline,
        dot_id_reporting_airline, iata_code_reporting_airline, originairportid, destairportid,
        originairportseqid, destairportseqid, origincitymarketid, destcitymarketid, originwac,
        destwac. Keep origin/dest (IATA codes), airline_name, and origin/dest lat/long.

        Execution: Add TEST_MODE to limit reads (READ_NROWS) and suppress plots for smoke tests.
        Write the final Parquet to ../data/output/flights_2024_clean_sampled.parquet.
    </data_engineering_criteria>
</prompt>
